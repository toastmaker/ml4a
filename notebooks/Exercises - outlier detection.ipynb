{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection - Using Supervised Learning\n",
    "- One-class SVM: https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM <br>\n",
    "- Supervised Random Forest: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate a simple dataset, and add some outliers to it\n",
    "\n",
    "size = 500\n",
    "mean_1 = np.array([0.1, 0.2])\n",
    "cov_1 = np.array([[0.01, 0.005], [0.005, 0.01]])\n",
    "X1 = np.random.multivariate_normal(mean_1, cov_1, size=size)\n",
    "\n",
    "mean_2 = np.array([0.3, 0.6])\n",
    "cov_2 = np.array([[0.01, -0.007], [-0.007, 0.01]])\n",
    "X2 = np.random.multivariate_normal(mean_2, cov_2, size=size)\n",
    "\n",
    "# outliers\n",
    "mean_3 = np.array([0.2, 0.4])\n",
    "cov_3 = np.array([[0.1, 0], [0, 0.1]])\n",
    "X3 = np.random.multivariate_normal(mean_3, cov_3, size=size/10) # size is smaller, because these are outliers!\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=\"r\", s=10)\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=\"b\", s=10)\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=\"g\", s=10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine the one-class SVM\n",
    "\n",
    "X = np.concatenate([X1, X2, X3])\n",
    "\n",
    "# shuffle the dataset before dividing it into training and test set\n",
    "choose_indices = numpy.arange(len(X))\n",
    "np.random.shuffle(choose_indices)\n",
    "train_indices = choose_indices[:len(choose_indices)/2]\n",
    "test_indices = choose_indices[len(choose_indices)/2:]\n",
    "X_train = X[train_indices, :]\n",
    "X_test = X[test_indices, :]\n",
    "print X_train.shape, X_test.shape\n",
    "\n",
    "# train a one-class SVM. Start with some hyper parameters\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.01)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# now we can predict for all the objects whether they belong to the class (1) or not (-1).\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# plot the decision boundries of the algorithm - where is the boundry between inliers and outliers?\n",
    "xx, yy = np.meshgrid(np.linspace(-0.4, 0.9, 500), np.linspace(-0.5, 1.3, 500))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "\n",
    "# plot the sample again, and color the points according to whether they are classified as inliers or outliers\n",
    "# yellow are inliers, purple are outliers\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_pred_train, s=10)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "Why do you think that the outlier detection did not work well? How can we improve it? <br>\n",
    "Play with different hyper-parameters for the SVM. How do they affect the outliers that you find? <br>\n",
    "Try to simulate a simpler and a more complex dataset. What type of outliers can you detect? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Random Forest to distinuguish between the two classes, and then we will see \n",
    "# whether the prediction probability can help us find the outliers. \n",
    "\n",
    "# we need to define labels, since we use supervised RF and we have two classes of objects\n",
    "y1 = np.zeros(len(X1))\n",
    "y2 = np.ones(len(X2))\n",
    "y3 = np.random.choice([0, 1], size=len(X3)) # assign random classes to outliers\n",
    "\n",
    "y = np.concatenate([y1, y2, y3])\n",
    "X = np.concatenate([X1, X2, X3])\n",
    "print X.shape, y.shape\n",
    "\n",
    "# divide the data into a training and a test set\n",
    "choose_indices = numpy.arange(len(X))\n",
    "np.random.shuffle(choose_indices)\n",
    "train_indices = choose_indices[:len(choose_indices)/2]\n",
    "test_indices = choose_indices[len(choose_indices)/2:]\n",
    "\n",
    "X_train = X[train_indices, :]\n",
    "X_test = X[test_indices, :]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[test_indices]\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# train a supervised RF to separate between the two groups\n",
    "# number of trees: 50\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine the prediction of the RF on the training and the test set\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# get the classification accuracy on the training and the test set\n",
    "score_train = clf.score(X_train, y_train)\n",
    "score_test = clf.score(X_test, y_test)\n",
    "print \"accuracy on training set: \", score_train\n",
    "print \"accuracy on test set: \", score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an amazing accuracy on test set!!!!!! :)\n",
    "\n",
    "Explain, what is happening!\n",
    "\n",
    "Now let us plot the decision boundaries of the classification probability, and then examine the type of outliers we find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create the decision boundries of the RF - this corresponds to the probability of the prediction\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-0.4, 0.9, 500), np.linspace(-0.5, 1.3, 500))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='bwr', alpha=0.3)\n",
    "plt.colorbar()\n",
    "\n",
    "# plot the measurements\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=\"k\", s=10)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=\"k\", s=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of an outlier\n",
    "In order to detect outliers, we must define a probability threshold with which we will define outliers. <br>\n",
    "The classification probability of p=0 corresponds to a probability of 1 for class A. <br>\n",
    "The classification probability of p=1 corresponds to a probability of 1 for class B. <br>\n",
    "Therefore, we will define outliers as objects that have a probability around 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_threshold = 0.3 # degree of outliers we will try to find\n",
    "proba_max = 0.5 + probability_threshold\n",
    "proba_min = 0.5 - probability_threshold\n",
    "# everything between these probabilities will be defined as an outlier!\n",
    "\n",
    "y_proba_train = clf.predict_proba(X_train)[:, 0]\n",
    "y_proba_test = clf.predict_proba(X_test)[:, 0]\n",
    "\n",
    "# plot the decision boundries\n",
    "xx, yy = np.meshgrid(np.linspace(-0.4, 0.9, 500), np.linspace(-0.5, 1.3, 500))\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='bwr', alpha=0.3)\n",
    "plt.colorbar()\n",
    "\n",
    "# plot the objects - black will be inliers, red will be outliers\n",
    "# inliers\n",
    "plt.scatter(X_train[((y_proba_train > proba_max) | (y_proba_train < proba_min)), 0], \n",
    "            X_train[((y_proba_train > proba_max) | (y_proba_train < proba_min)), 1], c=\"k\", s=10)\n",
    "plt.scatter(X_test[((y_proba_test > proba_max) | (y_proba_test < proba_min)), 0], \n",
    "            X_test[((y_proba_test > proba_max) | (y_proba_test < proba_min)), 1], c=\"k\", s=10)\n",
    "# outliers\n",
    "plt.scatter(X_train[((y_proba_train < proba_max) & (y_proba_train > proba_min)), 0], \n",
    "            X_train[((y_proba_train < proba_max) & (y_proba_train > proba_min)), 1], c=\"r\", s=10)\n",
    "plt.scatter(X_test[((y_proba_test < proba_max) & (y_proba_test > proba_min)), 0], \n",
    "            X_test[((y_proba_test < proba_max) & (y_proba_test > proba_min)), 1], c=\"r\", s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Do you think that this is a good outlier detection algorithm? How can you improve it? <br>\n",
    "Play with the probability threshold, which value will give you the best outliers? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Simulate a more complex dataset, perhaps with more classes and more features. <br>\n",
    "For that, you can use **sklearn** make_classification: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html <br>\n",
    "This can be used to simulate complex dataset with many features.\n",
    "Then, play with the Random Forest hyper-parameters, and search for the hyper-parameters that result in the highest accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection - Isolation forests\n",
    "Here we will examine the output of Isolation Forest, which is an unsupervised learning algorithm for finding outliers. <br>\n",
    "We will use **sklearn** implementation of Isolation Forests: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct our dataset, now we do not need labels!\n",
    "X = np.concatenate([X1, X2, X3])\n",
    "\n",
    "# Isolation Forests\n",
    "# contamination: percentage of outliers\n",
    "clf = IsolationForest(max_samples=100, contamination=0.05)\n",
    "clf.fit(X)\n",
    "\n",
    "# plot decision boundries \n",
    "xx, yy = np.meshgrid(np.linspace(-0.6, 1.1, 500), np.linspace(-0.5, 1.35, 500))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "plt.colorbar()\n",
    "\n",
    "# plot the acutal objects\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"k\", s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Define the threshold below which an object will be defined as an outlier. <br>\n",
    "Use different thresholds, what is the best threshold? How can you define it automaticaly? <br>\n",
    "Use the decision_function to examine these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
